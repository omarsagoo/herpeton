{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AAI-521 Group 6 Group Project"
      ],
      "metadata": {
        "id": "M-5CBNxOwhKO"
      },
      "id": "M-5CBNxOwhKO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# herpeton"
      ],
      "metadata": {
        "id": "9sZEOgrrwrFA"
      },
      "id": "9sZEOgrrwrFA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Friendly Notebook"
      ],
      "metadata": {
        "id": "acgPwAwdYa4B"
      },
      "id": "acgPwAwdYa4B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "027bbacf",
        "outputId": "8c782314-be06-4236-d273-a911eed6c0ed"
      },
      "source": [
        "#@title Setup for TPU\n",
        "print(\"Installing torch_xla and torchvision...\")\n",
        "!pip install torch_xla torchvision -f https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla/torch_xla-*.whl\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.runtime as runtime # Added import for runtime\n",
        "import torch_xla # Added direct import for torch_xla.device()\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image, UnidentifiedImageError # Import UnidentifiedImageError\n",
        "\n",
        "!pip -q install timm\n",
        "import timm\n"
      ],
      "id": "027bbacf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torch_xla and torchvision...\n",
            "Looking in links: https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla/torch_xla-*.whl\n",
            "Requirement already satisfied: torch_xla in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from torch_xla) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_xla) (2.32.4)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cpu)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.12.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_xla) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configure the device\n",
        "try:\n",
        "    # Attempt to get TPU device\n",
        "    device = torch_xla.device()\n",
        "    print(f\"Device: {device} (TPU)\")\n",
        "except RuntimeError:\n",
        "    # Fallback to CUDA if TPU is not available\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"Device: {device} (CUDA)\")\n",
        "    else:\n",
        "        # Fallback to CPU if neither TPU nor CUDA is available\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(f\"Device: {device} (CPU)\")\n",
        "\n",
        "# Verify device setup\n",
        "print(f\"Current device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F85VXSf7Wy6n",
        "outputId": "40fe5b18-6e26-44bb-9d3e-6f61f0998bd2"
      },
      "id": "F85VXSf7Wy6n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: xla:0 (TPU)\n",
            "Current device: xla:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "def create_warmup_scheduler(optimizer, warmup_steps: int):\n",
        "    def lr_lambda(step: int):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        return 1.0\n",
        "    return LambdaLR(optimizer, lr_lambda)"
      ],
      "metadata": {
        "id": "2lIeYoRNWP6t"
      },
      "id": "2lIeYoRNWP6t",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 40\n",
        "LR = 0.0005\n",
        "WEIGHT_DECAY = 0.0004\n",
        "WARMUP_STEPS = 5000"
      ],
      "metadata": {
        "id": "dkpBIPVxWQUH"
      },
      "id": "dkpBIPVxWQUH",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "BASE_PATH = \"/content/drive/MyDrive/herpeton//data/biotrove_train\"\n",
        "metadata_path = os.path.join(BASE_PATH, \"reptilia_dataset_processed.csv\")\n",
        "\n",
        "df = pd.read_csv(metadata_path)\n",
        "df[\"label_id\"], label_names = pd.factorize(df[\"species\"])\n",
        "num_classes = len(label_names)\n",
        "\n",
        "train_df = df[df[\"split\"] == \"train\"].copy()\n",
        "val_df = df[df[\"split\"] == \"val\"].copy()\n",
        "test_df = df[df[\"split\"] == \"test\"].copy()\n",
        "\n",
        "train_df = train_df.dropna(subset=['image_path_fixed']).reset_index(drop=True)\n",
        "val_df = val_df.dropna(subset=['image_path_fixed']).reset_index(drop=True)\n",
        "test_df = test_df.dropna(subset=['image_path_fixed']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hEzDOXGNWpmH"
      },
      "id": "hEzDOXGNWpmH",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "20a955bbe54643b8b7f524bc2964af70",
            "9a10ecb897a0441abefe739d149fe2bb",
            "edab15fc5bb744e98f2a084ef50b4679",
            "6d3497f0e89d469fa799b999978a2209",
            "75ae7538f7cf4c0aa45f45ebb73130af",
            "0d730e6ec45646a2b3486d9f17afb1c0",
            "8fa1b0efb3884c6ca4c33070e5a734c2",
            "3ce69389a1914ab89e60f7cf3d9eb213",
            "136a769caeff44709f19de2e372cb79e",
            "6f81b2b51aaf4b318ed6221654bfd840",
            "f7dadf28efa4479c8893778acc441d66"
          ]
        },
        "id": "2fe48d3c",
        "outputId": "a5c4dd00-c226-490c-f58c-f93010972a2c"
      },
      "source": [
        "#@title ResNet50\n",
        "\n",
        "# Helper function\n",
        "def create_warmup_scheduler(optimizer, warmup_steps: int):\n",
        "    def lr_lambda(step: int):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        return 1.0\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Custom collate function to handle None values from __getitem__\n",
        "def collate_fn_skip_none(batch):\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    if not batch: # If the batch is empty after filtering\n",
        "        # Return empty tensors that can be handled by the training/eval loop\n",
        "        return torch.tensor([]), torch.tensor([])\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "class ReptiliaDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row[\"image_path_fixed\"]\n",
        "        label = int(row[\"label_id\"])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except (IOError, OSError, UnidentifiedImageError) as e: # Catch specific IO, OS, and image loading errors\n",
        "            print(f\"Error loading {img_path}: {e}. Skipping this sample.\", flush=True)\n",
        "            return None # Return None for problematic samples\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((image_size, image_size)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((image_size, image_size)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "requested_batch_size = 64\n",
        "\n",
        "flags = {\n",
        "    \"EPOCHS\": EPOCHS,\n",
        "    \"LR\": LR,\n",
        "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
        "    \"WARMUP_STEPS\": WARMUP_STEPS,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"train_df\": train_df,\n",
        "    \"val_df\": val_df,\n",
        "    \"test_df\": test_df,\n",
        "    \"train_transform\": train_transform,\n",
        "    \"val_transform\": val_transform,\n",
        "    \"ReptiliaDataset\": ReptiliaDataset,\n",
        "    \"requested_batch_size\": requested_batch_size,\n",
        "    \"create_warmup_scheduler\": create_warmup_scheduler,\n",
        "    \"collate_fn_skip_none\": collate_fn_skip_none, # Pass the collate_fn to flags\n",
        "}\n",
        "\n",
        "def _mp_fn(index, flags):\n",
        "    EPOCHS = flags[\"EPOCHS\"]\n",
        "    LR = flags[\"LR\"]\n",
        "    WEIGHT_DECAY = flags[\"WEIGHT_DECAY\"]\n",
        "    WARMUP_STEPS = flags[\"WARMUP_STEPS\"]\n",
        "    num_classes = flags[\"num_classes\"]\n",
        "    train_df = flags[\"train_df\"]\n",
        "    val_df = flags[\"val_df\"]\n",
        "    test_df = flags[\"test_df\"]\n",
        "    train_transform = flags[\"train_transform\"]\n",
        "    val_transform = flags[\"val_transform\"]\n",
        "    ReptiliaDataset = flags[\"ReptiliaDataset\"]\n",
        "    BATCH_SIZE_PER_CORE = flags[\"requested_batch_size\"]\n",
        "    create_warmup_scheduler = flags[\"create_warmup_scheduler\"]\n",
        "    collate_fn_skip_none = flags[\"collate_fn_skip_none\"]\n",
        "\n",
        "    device = torch_xla.device()\n",
        "    print(f\"[{index}] Device: {device}\")\n",
        "\n",
        "    xm.rendezvous(\"init_dist_resnet_tpu\")\n",
        "\n",
        "    train_dataset = ReptiliaDataset(train_df, transform=train_transform)\n",
        "    val_dataset = ReptiliaDataset(val_df, transform=val_transform)\n",
        "    test_dataset = ReptiliaDataset(test_df, transform=val_transform)\n",
        "\n",
        "    # Use pl.MpDeviceLoader from parallel_loader\n",
        "    train_loader = pl.MpDeviceLoader(\n",
        "        DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=BATCH_SIZE_PER_CORE,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            drop_last=False, # Changed to False to allow collate_fn to handle uneven batches\n",
        "            collate_fn=collate_fn_skip_none # Add custom collate_fn\n",
        "        ),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    val_loader = pl.MpDeviceLoader(\n",
        "        DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=BATCH_SIZE_PER_CORE,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            drop_last=False,\n",
        "            collate_fn=collate_fn_skip_none # Add custom collate_fn\n",
        "        ),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    test_loader = pl.MpDeviceLoader(\n",
        "        DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=BATCH_SIZE_PER_CORE,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            drop_last=False,\n",
        "            collate_fn=collate_fn_skip_none # Add custom collate_fn\n",
        "        ),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    if runtime.global_ordinal() == 0: # Changed xm.get_ordinal() to runtime.global_ordinal()\n",
        "        print(f\"[{index}] Dataloader batches (per core): Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")\n",
        "\n",
        "    def train_one_epoch_xla(model, loader, optimizer, scheduler, epoch, total_steps_done, device, use_bf16=True):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        # MpDeviceLoader handles device placement; iterate directly\n",
        "        for images, labels in tqdm(loader, desc=f\"[{index}] Train Epoch {epoch}\", disable=(runtime.global_ordinal() != 0)): # Changed xm.get_ordinal()\n",
        "            if images.numel() == 0: # Skip if batch is empty\n",
        "                continue\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs = model(images)\n",
        "            loss = nn.functional.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            n_batches += 1\n",
        "            total_steps_done += 1\n",
        "\n",
        "        reduced_running_loss = xm.mesh_reduce('train_loss_reduce', running_loss, np.sum)\n",
        "        reduced_n_batches = xm.mesh_reduce('train_batches_reduce', n_batches, np.sum)\n",
        "        avg_loss = reduced_running_loss / max(1, reduced_n_batches)\n",
        "\n",
        "        if runtime.global_ordinal() == 0: # Changed xm.get_ordinal()\n",
        "            print(f\"Epoch {epoch} - Train Loss: {avg_loss:.4f}\")\n",
        "        return total_steps_done, avg_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_xla(model, loader, device):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for images, labels in tqdm(loader, desc=f\"[{index}] Eval\", disable=(runtime.global_ordinal() != 0)): # Changed xm.get_ordinal()\n",
        "            if images.numel() == 0: # Skip if batch is empty\n",
        "                continue\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = nn.functional.cross_entropy(outputs, labels)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        reduced_total_loss = xm.mesh_reduce(\"eval_loss_reduce\", total_loss, np.sum)\n",
        "        reduced_num_batches = xm.mesh_reduce(\"eval_batches_reduce\", num_batches, np.sum)\n",
        "\n",
        "        all_preds_tensor = torch.tensor(all_preds, device=device)\n",
        "        all_labels_tensor = torch.tensor(all_labels, device=device)\n",
        "        global_all_preds = xmp.all_gather(all_preds_tensor).cpu().numpy()\n",
        "        global_all_labels = xmp.all_gather(all_labels_tensor).cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(global_all_labels, global_all_preds)\n",
        "        avg_loss = reduced_total_loss / max(1, reduced_num_batches)\n",
        "        return acc, avg_loss\n",
        "\n",
        "    model = timm.create_model(\"resnet50\", pretrained=True, num_classes=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = create_warmup_scheduler(optimizer, warmup_steps=WARMUP_STEPS)\n",
        "\n",
        "    history = {\"epoch\": [], \"val_acc\": [], \"val_loss\": [], \"train_loss\": []}\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        global_step, train_loss = train_one_epoch_xla(\n",
        "            model, train_loader, optimizer, scheduler, epoch, total_steps_done=global_step, device=device, use_bf16=True\n",
        "        )\n",
        "        val_acc, val_loss = evaluate_xla(model, val_loader, device)\n",
        "\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        if runtime.global_ordinal() == 0: # Changed xm.get_ordinal()\n",
        "            print(f\"Epoch {epoch} - Val Accuracy: {val_acc:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if runtime.global_ordinal() == 0: # Changed xm.get_ordinal()\n",
        "        return history\n",
        "\n",
        "print(\"Launching ResNet50 training on TPU cores...\")\n",
        "history_resnet_tpu = xmp.spawn(_mp_fn, args=(flags,), nprocs=len(xm.get_xla_supported_devices()))\n",
        "print(\"TPU training complete.\")"
      ],
      "id": "2fe48d3c",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching ResNet50 training on TPU cores...\n",
            "[0] Device: xla:0\n",
            "[0] Dataloader batches (per core): Train=217, Val=31, Test=28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[0] Train Epoch 1:   0%|          | 0/217 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a955bbe54643b8b7f524bc2964af70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-890749335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching ResNet50 training on TPU cores...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mhistory_resnet_tpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xla_supported_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TPU training complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \"\"\"\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpjrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m_run_singleprocess\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \"\"\"\n\u001b[1;32m     96\u001b[0m   \u001b[0minitialize_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_ordinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-890749335.py\u001b[0m in \u001b[0;36m_mp_fn\u001b[0;34m(index, flags)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         global_step, train_loss = train_one_epoch_xla(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bf16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-890749335.py\u001b[0m in \u001b[0;36mtrain_one_epoch_xla\u001b[0;34m(model, loader, optimizer, scheduler, epoch, total_steps_done, device, use_bf16)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# MpDeviceLoader handles device placement; iterate directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"[{index}] Train Epoch {epoch}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_ordinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Changed xm.get_ordinal()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Skip if batch is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     self._parallel_loader = ParallelLoader(self._loader, [self._device],\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "BivlapWQZHIW"
      },
      "id": "BivlapWQZHIW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20a955bbe54643b8b7f524bc2964af70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a10ecb897a0441abefe739d149fe2bb",
              "IPY_MODEL_edab15fc5bb744e98f2a084ef50b4679",
              "IPY_MODEL_6d3497f0e89d469fa799b999978a2209"
            ],
            "layout": "IPY_MODEL_75ae7538f7cf4c0aa45f45ebb73130af"
          }
        },
        "9a10ecb897a0441abefe739d149fe2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d730e6ec45646a2b3486d9f17afb1c0",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa1b0efb3884c6ca4c33070e5a734c2",
            "value": "[0] Train Epoch 1:   0%"
          }
        },
        "edab15fc5bb744e98f2a084ef50b4679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce69389a1914ab89e60f7cf3d9eb213",
            "max": 217,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136a769caeff44709f19de2e372cb79e",
            "value": 0
          }
        },
        "6d3497f0e89d469fa799b999978a2209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f81b2b51aaf4b318ed6221654bfd840",
            "placeholder": "​",
            "style": "IPY_MODEL_f7dadf28efa4479c8893778acc441d66",
            "value": " 0/217 [00:29&lt;?, ?it/s]"
          }
        },
        "75ae7538f7cf4c0aa45f45ebb73130af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d730e6ec45646a2b3486d9f17afb1c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa1b0efb3884c6ca4c33070e5a734c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ce69389a1914ab89e60f7cf3d9eb213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136a769caeff44709f19de2e372cb79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f81b2b51aaf4b318ed6221654bfd840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dadf28efa4479c8893778acc441d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}