{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarsagoo/herpeton/blob/Carrie/notebook_colab_Updated%20EDA%20and%20Model%202%20and%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AAI-521 Group 6 Group Project"
      ],
      "metadata": {
        "id": "M-5CBNxOwhKO"
      },
      "id": "M-5CBNxOwhKO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# herpeton"
      ],
      "metadata": {
        "id": "9sZEOgrrwrFA"
      },
      "id": "9sZEOgrrwrFA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Friendly Notebook - EDA"
      ],
      "metadata": {
        "id": "acgPwAwdYa4B"
      },
      "id": "acgPwAwdYa4B"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environment Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "!pip install imagehash\n",
        "import imagehash\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Show plots inline\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install squarify > /dev/null\n",
        "import squarify\n",
        "\n",
        "!pip install plotly > /dev/null\n",
        "import plotly.express as px\n",
        "\n",
        "!pip install wordcloud > /dev/null\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "P5JEs1ktYJVr"
      },
      "id": "P5JEs1ktYJVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "CURATE_TO_DRIVE = True\n",
        "\n",
        "if CURATE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_OUT = \"/content/drive/MyDrive/herpeton/data/biotrove_train\"\n",
        "else:\n",
        "    BASE_OUT = \"/content/herpeton\"\n",
        "\n",
        "os.makedirs(BASE_OUT, exist_ok=True)\n",
        "\n",
        "# Controls to keep runtime reasonable\n",
        "MAX_RECORDS = 15000          # hard cap across the stream (increase if you have time/compute)\n",
        "SAMPLE_PER_SPECIES = 50    # images per species to collect/curate/EDA\n",
        "REPORT_DIR = os.path.join(BASE_OUT, \"_reports\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output base: {BASE_OUT}\")\n",
        "print(f\"Reports   : {REPORT_DIR}\")\n",
        "print(f\"Limits    : MAX_RECORDS={MAX_RECORDS}, SAMPLE_PER_SPECIES={SAMPLE_PER_SPECIES}\")"
      ],
      "metadata": {
        "id": "jEsm8rUUYW-3"
      },
      "id": "jEsm8rUUYW-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Processed Dataset\n",
        "\n",
        "# Paths to processed dataset\n",
        "BASE_PATH = \"/content/drive/MyDrive/herpeton//data/biotrove_train\"\n",
        "IMG_DIR = os.path.join(BASE_PATH, \"images_reptilia\")\n",
        "\n",
        "# Load all CSVs\n",
        "csv_files = [f for f in os.listdir(BASE_PATH) if f.endswith(\".csv\")]\n",
        "csv_files\n",
        "\n",
        "# Load CSVs\n",
        "dfs = [pd.read_csv(os.path.join(BASE_PATH, f)) for f in csv_files]\n",
        "\n",
        "# If there are multiple metadata CSVs, merge them on common columns\n",
        "# Otherwise take the first one\n",
        "if len(dfs) == 1:\n",
        "    df = dfs[0]\n",
        "else:\n",
        "    # Try automatic merge: assumes shared columns\n",
        "    common_cols = list(set(dfs[0].columns).intersection(*[set(d.columns) for d in dfs]))\n",
        "    df = dfs[0]\n",
        "    for d in dfs[1:]:\n",
        "        df = df.merge(d, on=common_cols, how=\"outer\")\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Q1eBt_XnclvU"
      },
      "id": "Q1eBt_XnclvU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Validate and Correct Image Paths\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/herpeton/biotrove_processed/images\"\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/herpeton/biotrove_processed/images\"\n",
        "\n",
        "def fix_path(p):\n",
        "    # Handle non-string inputs (e.g., NaN values)\n",
        "    if not isinstance(p, str):\n",
        "        return np.nan\n",
        "\n",
        "    # Extract just the filename (e.g., \"69365.jpg\")\n",
        "    filename = os.path.basename(p)\n",
        "\n",
        "    # Build the full path\n",
        "    full_path = os.path.join(BASE_DIR, filename)\n",
        "\n",
        "    return full_path\n",
        "\n",
        "df[\"image_path_fixed\"] = df[\"image_path\"].apply(fix_path)\n",
        "\n",
        "# Test\n",
        "df[[\"image_path\", \"image_path_fixed\"]].head()"
      ],
      "metadata": {
        "id": "lOi1NaNzdgXW"
      },
      "id": "lOi1NaNzdgXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate the width and height for each image and add them to df\n",
        "\n",
        "if 'width' not in df.columns or 'height' not in df.columns:\n",
        "    print(\"Calculating image dimensions (width, height)... This might take a moment.\")\n",
        "    widths = []\n",
        "    heights = []\n",
        "    for img_path in tqdm(df[\"image_path_fixed\"], desc=\"Processing images\"):\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                widths.append(img.width)\n",
        "                heights.append(img.height)\n",
        "        except (FileNotFoundError, UnidentifiedImageError, Exception) as e:\n",
        "            # Handle cases where image file might be missing or corrupted\n",
        "            widths.append(np.nan)\n",
        "            heights.append(np.nan)\n",
        "    df[\"width\"] = widths\n",
        "    df[\"height\"] = heights\n",
        "    print(\"Image dimensions added to DataFrame.\")"
      ],
      "metadata": {
        "id": "ibR19CuipS8C"
      },
      "id": "ibR19CuipS8C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "cWaYLpk4sGtL"
      },
      "id": "cWaYLpk4sGtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "75SiiCgmsJcZ"
      },
      "id": "75SiiCgmsJcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "XPxZkflWsOnU"
      },
      "id": "XPxZkflWsOnU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function to show random images\n",
        "def show_random_images(df_subset, n=12):\n",
        "    if 'image_path_fixed' not in df_subset.columns:\n",
        "        print(\"DataFrame does not contain an 'image_path_fixed' column.\")\n",
        "        return\n",
        "\n",
        "    num_samples = min(n, len(df_subset))\n",
        "    if num_samples == 0:\n",
        "        print(\"No images to display from the provided DataFrame subset.\")\n",
        "        return\n",
        "\n",
        "    # Use random_state for reproducibility in sampling\n",
        "    sample_paths_relative = df_subset[\"image_path_fixed\"].sample(num_samples, random_state=42).tolist()\n",
        "\n",
        "    cols = 4\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "\n",
        "    plt.figure(figsize=(15, rows * 4))\n",
        "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
        "\n",
        "    for i, relative_path in enumerate(sample_paths_relative):\n",
        "        # Construct the full absolute path using BASE_OUT\n",
        "        full_path = os.path.join(BASE_OUT, relative_path)\n",
        "        try:\n",
        "            img = Image.open(full_path)\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {full_path}\")\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.text(0.5, 0.5, \"Image Not Found\", horizontalalignment='center', verticalalignment='center', fontsize=10)\n",
        "            plt.axis(\"off\")\n",
        "        except UnidentifiedImageError:\n",
        "            print(f\"Cannot identify image file: {full_path}\")\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.text(0.5, 0.5, \"Corrupt Image\", horizontalalignment='center', verticalalignment='center', fontsize=10)\n",
        "            plt.axis(\"off\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {full_path}: {e}\")\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.text(0.5, 0.5, f\"Error: {str(e)[:20]}...\", horizontalalignment='center', verticalalignment='center', fontsize=10)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Sample Images\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dZWWSk1Wfcbq"
      },
      "id": "dZWWSk1Wfcbq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    \"unique_families\": [df[\"family\"].nunique()],\n",
        "    \"unique_species\": [df[\"species\"].nunique()],\n",
        "    \"unique_common_names\": [df[\"common_name\"].nunique()],\n",
        "    \"rows_total\": [len(df)]\n",
        "})\n",
        "\n",
        "summary"
      ],
      "metadata": {
        "id": "JX7HGwzCwDFJ"
      },
      "id": "JX7HGwzCwDFJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Species Distribution\n",
        "species_counts = df[\"species\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "species_counts.head(20).plot(kind=\"barh\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 20 Reptile Species by Image Count\")\n",
        "plt.xlabel(\"Images\")\n",
        "plt.ylabel(\"Species\")\n",
        "plt.show()\n",
        "\n",
        "species_counts.describe()"
      ],
      "metadata": {
        "id": "-4dgza-8eXOn"
      },
      "id": "-4dgza-8eXOn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top 20 Families\n",
        "plt.figure(figsize=(12,6))\n",
        "df[\"family\"].value_counts().head(20).plot(kind=\"bar\")\n",
        "plt.title(\"Top 20 Families\")\n",
        "plt.xlabel(\"Family\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B3Z3i9sVrr8f"
      },
      "id": "B3Z3i9sVrr8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top 20 Species\n",
        "plt.figure(figsize=(12,6))\n",
        "df[\"species\"].value_counts().head(20).plot(kind=\"bar\")\n",
        "plt.title(\"Top 20 Species\")\n",
        "plt.xlabel(\"Species\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y6JPS-8Xr5SP"
      },
      "id": "y6JPS-8Xr5SP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top 20 Common Names\n",
        "plt.figure(figsize=(12,6))\n",
        "df[\"common_name\"].value_counts().head(20).plot(kind=\"bar\")\n",
        "plt.title(\"Top 20 Common Names\")\n",
        "plt.xlabel(\"Common Name\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cgKMEYvDsjv_"
      },
      "id": "cgKMEYvDsjv_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Species per Family\n",
        "family_species_counts = df.groupby(\"family\")[\"species\"].nunique().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "family_species_counts.head(20).plot(kind='bar')\n",
        "plt.title(\"Number of Unique Species Per Family\")\n",
        "plt.xlabel(\"Family\")\n",
        "plt.ylabel(\"Unique Species Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDKKJYI-tqMd"
      },
      "id": "hDKKJYI-tqMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Boxplot - Species\n",
        "species_counts = df.groupby([\"family\", \"species\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=species_counts, x=\"family\", y=\"count\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribution of Image Counts per Species Within Each Family\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2fYb2BrXvqg9"
      },
      "id": "2fYb2BrXvqg9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Treemap - Family Distribution\n",
        "family_counts = df[\"family\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "squarify.plot(sizes=family_counts.values, label=family_counts.index, alpha=0.8)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Family Distribution Treemap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tEsK5iz1uH5k"
      },
      "id": "tEsK5iz1uH5k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sunburst\n",
        "\n",
        "fig = px.sunburst(\n",
        "    df,\n",
        "    path=[\"family\", \"genus\", \"species\"],\n",
        "    values=None,\n",
        "    title=\"Taxonomy Sunburst Chart (Family → Genus → Species)\"\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tPUbii6Eu5Hc"
      },
      "id": "tPUbii6Eu5Hc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wordcloud - Common Names\n",
        "text = \" \".join(df[\"common_name\"].dropna().astype(str).tolist())\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", width=800, height=400).generate(text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud: Common Names\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cEQ9NQGau5TJ"
      },
      "id": "cEQ9NQGau5TJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wordcloud - Family Names\n",
        "text = \" \".join(df[\"family\"].dropna().astype(str).tolist())\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", width=800, height=400).generate(text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud: Reptile Families\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o1V1e0Ybu5db"
      },
      "id": "o1V1e0Ybu5db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Visualize Sample Images From a Specific Species\n",
        "species_list = df[\"species\"].unique()\n",
        "species_list[:20]\n",
        "\n",
        "def show_species_samples(df, species, n=12):\n",
        "    subset = df[df[\"species\"] == species]\n",
        "    sample_paths = subset[\"image_path_fixed\"].sample(min(n, len(subset))).tolist()\n",
        "\n",
        "    cols = 4\n",
        "    rows = n // cols + 1\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i, path in enumerate(sample_paths):\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "        except:\n",
        "            pass\n",
        "    plt.suptitle(f\"Samples of Species: {species}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "show_species_samples(df, species=species_list[0], n=12)"
      ],
      "metadata": {
        "id": "_v29MU34f1ap"
      },
      "id": "_v29MU34f1ap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Show Samples from the MOST COMMON Species\n",
        "top_species = df[\"species\"].value_counts().idxmax()\n",
        "print(\"Most common species:\", top_species)\n",
        "\n",
        "show_species_samples(df, species=top_species, n=12)"
      ],
      "metadata": {
        "id": "ITXRbLzogKNW"
      },
      "id": "ITXRbLzogKNW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Show Samples from the RAREST Species\n",
        "rare_species = df[\"species\"].value_counts().idxmin()\n",
        "print(\"Rarest species:\", rare_species)\n",
        "\n",
        "show_species_samples(df, species=rare_species, n=12)"
      ],
      "metadata": {
        "id": "Ut2ddTZlgYXo"
      },
      "id": "Ut2ddTZlgYXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Random Species Sample\n",
        "import random\n",
        "def sample_random_species(df):\n",
        "    species = random.choice(df[\"species\"].unique())\n",
        "    print(\"Random species selected:\", species)\n",
        "    show_species_samples(df, species=species, n=12)\n",
        "\n",
        "# Try multiple times:\n",
        "sample_random_species(df)"
      ],
      "metadata": {
        "id": "66hLHjdbgmbd"
      },
      "id": "66hLHjdbgmbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Image Grid\n",
        "def mosaic(df, n=25):\n",
        "    sample_paths = df[\"image_path_fixed\"].sample(n).tolist()\n",
        "    plt.figure(figsize=(14, 14))\n",
        "\n",
        "    for i, p in enumerate(sample_paths):\n",
        "        try:\n",
        "            img = Image.open(p)\n",
        "            plt.subplot(5, 5, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    plt.suptitle(\"Random Mosaic of Reptilia Images\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "mosaic(df, n=25)"
      ],
      "metadata": {
        "id": "FaxBD0Eggucd"
      },
      "id": "FaxBD0Eggucd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Check for Outliers - Small\n",
        "\n",
        "df_small = df.sort_values(by=[\"width\", \"height\"]).head(10)\n",
        "df_small\n",
        "\n",
        "show_random_images(df_small, n=min(10, len(df_small)))"
      ],
      "metadata": {
        "id": "w__Xrf6shM2K"
      },
      "id": "w__Xrf6shM2K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Check for Outliers - Wide\n",
        "df[\"aspect_ratio\"] = df[\"width\"] / df[\"height\"]\n",
        "\n",
        "df_wide = df.sort_values(by=\"aspect_ratio\", ascending=False).head(10)\n",
        "\n",
        "print(\"Wide images:\")\n",
        "display(df_wide)\n",
        "\n",
        "show_random_images(df_wide, n=min(10, len(df_wide)))\n"
      ],
      "metadata": {
        "id": "DptwCyuns0PY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DptwCyuns0PY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tETLFOzilpN"
      },
      "id": "0tETLFOzilpN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Check for Outliers - Tall\n",
        "df[\"aspect_ratio\"] = df[\"width\"] / df[\"height\"]\n",
        "\n",
        "df_tall = df.sort_values(by=\"aspect_ratio\", ascending=True).head(10)\n",
        "\n",
        "print(\"Tall images:\")\n",
        "display(df_tall)\n",
        "\n",
        "show_random_images(df_tall, n=min(10, len(df_tall)))"
      ],
      "metadata": {
        "id": "d35vZnNwtCX7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "d35vZnNwtCX7"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "#@title Check for Duplicates\n",
        "\n",
        "# Define function to calculate perceptual hash (phash)\n",
        "def calculate_phash(image_path):\n",
        "    if pd.isna(image_path):\n",
        "        return None\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        # Convert to RGB to avoid issues with different modes (e.g., RGBA, P)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        return str(imagehash.phash(img))\n",
        "    except (FileNotFoundError, UnidentifiedImageError, Exception) as e:\n",
        "        return None\n",
        "\n",
        "# Calculate phash for all images\n",
        "# Only calculate if 'phash' column does not exist to avoid re-computation\n",
        "if 'phash' not in df.columns:\n",
        "    print(\"Calculating perceptual hashes (phash)... This might take a moment.\")\n",
        "    tqdm.pandas(desc=\"Calculating phash\") # Enable progress bar for pandas apply\n",
        "    df[\"phash\"] = df[\"image_path_fixed\"].progress_apply(calculate_phash)\n",
        "    print(\"Perceptual hashes added to DataFrame.\")\n",
        "\n",
        "# Find duplicate groups based on phash\n",
        "phash_counts = df[\"phash\"].value_counts()\n",
        "duplicate_hashes = phash_counts[phash_counts > 1].index\n",
        "\n",
        "dup_groups = df[df[\"phash\"].isin(duplicate_hashes)].sort_values(by=\"phash\")\n",
        "\n",
        "if not dup_groups.empty:\n",
        "    print(f\"Found {len(duplicate_hashes)} groups of duplicate images.\")\n",
        "    dup_sample = dup_groups.groupby(\"phash\").head(2)  # two per duplicate group\n",
        "    show_random_images(dup_sample, n=min(12, len(dup_sample)))\n",
        "else:\n",
        "    print(\"No duplicate images found based on perceptual hashing.\")\n"
      ],
      "metadata": {
        "id": "QgxwlqaciG3E"
      },
      "id": "QgxwlqaciG3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6c4AzjiumAj"
      },
      "id": "u6c4AzjiumAj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "krNsKsAZumnF"
      },
      "id": "krNsKsAZumnF"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environment Setup\n",
        "!pip -q install timm\n",
        "\n",
        "import os\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import timm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "aD1DPhihv2EM"
      },
      "id": "aD1DPhihv2EM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive & Load Metadata\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "metadata_path = (\n",
        "    \"/content/drive/MyDrive/herpeton/biotrove_reptilia_curated/\"\n",
        "    \"metadata/reptilia_curated_metadata.csv\"\n",
        ")\n",
        "\n",
        "df = pd.read_csv(metadata_path)\n",
        "print(\"Metadata shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "r9JvteR_wwYK"
      },
      "id": "r9JvteR_wwYK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Encode Labels & Train/Val Split\n",
        "# Use 'species' as label\n",
        "assert \"species\" in df.columns, \"Expected 'species' column in metadata.\"\n",
        "assert \"saved_file\" in df.columns, \"Expected 'saved_file' column with image paths.\"\n",
        "\n",
        "# Encode species as integer labels\n",
        "df[\"label_id\"], label_names = pd.factorize(df[\"species\"])\n",
        "num_classes = len(label_names)\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label_id\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n"
      ],
      "metadata": {
        "id": "AYzCaoC1zqKg"
      },
      "id": "AYzCaoC1zqKg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset & Transforms\n",
        "class ReptiliaDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row[\"saved_file\"]\n",
        "        label = int(row[\"label_id\"])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Image size compatible with ResNet50\n",
        "image_size = 224\n",
        "\n",
        "train_transform = T.Compose(\n",
        "    [\n",
        "        T.Resize((image_size, image_size)),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomRotation(10),\n",
        "        T.ColorJitter(\n",
        "            brightness=0.2,\n",
        "            contrast=0.2,\n",
        "            saturation=0.2,\n",
        "            hue=0.02,\n",
        "        ),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = T.Compose(\n",
        "    [\n",
        "        T.Resize((image_size, image_size)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = ReptiliaDataset(train_df, transform=train_transform)\n",
        "val_dataset = ReptiliaDataset(val_df, transform=val_transform)\n"
      ],
      "metadata": {
        "id": "A63C_fGYz163"
      },
      "id": "A63C_fGYz163",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataloaders\n",
        "# Your requested batch size:\n",
        "requested_batch_size = 4096\n",
        "\n",
        "# This may be too large for Colab; you can lower manually if you get OOM.\n",
        "BATCH_SIZE = requested_batch_size\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "len(train_loader), len(val_loader)\n"
      ],
      "metadata": {
        "id": "gwUDqT380G9g"
      },
      "id": "gwUDqT380G9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Utilities\n",
        "def create_warmup_scheduler(optimizer, warmup_steps: int):\n",
        "    \"\"\"\n",
        "    Linear warmup from 0 -> 1 over warmup_steps, then stays at 1.\n",
        "    \"\"\"\n",
        "    def lr_lambda(step: int):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        return 1.0\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[LambdaLR],\n",
        "    epoch: int,\n",
        "    total_steps_done: int,\n",
        "    use_bf16: bool = True,\n",
        ") -> int:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    # Check bf16 capability\n",
        "    can_bf16 = use_bf16 and torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=not can_bf16)\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=f\"Train Epoch {epoch}\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if can_bf16:\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                outputs = model(images)\n",
        "                loss = nn.functional.cross_entropy(outputs, labels)\n",
        "        else:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = nn.functional.cross_entropy(outputs, labels)\n",
        "\n",
        "        # backward + step\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        n_batches += 1\n",
        "        total_steps_done += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, n_batches)\n",
        "    print(f\"Epoch {epoch} - Train Loss: {avg_loss:.4f}\")\n",
        "    return total_steps_done\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    all_preds: List[int] = []\n",
        "    all_labels: List[int] = []\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Eval\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return acc, 0.0  # placeholder for loss if needed\n"
      ],
      "metadata": {
        "id": "mi220Cyc0PsD"
      },
      "id": "mi220Cyc0PsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1  - ResNet50 Baseline"
      ],
      "metadata": {
        "id": "voTRbqSxu72G"
      },
      "id": "voTRbqSxu72G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for Model 1\n",
        "EPOCHS = 40\n",
        "LR = 0.0005\n",
        "WEIGHT_DECAY = 0.0004\n",
        "WARMUP_STEPS = 5000  # global steps\n",
        "\n",
        "# Build ResNet50 (using timm for consistency)\n",
        "model_resnet = timm.create_model(\n",
        "    \"resnet50\",\n",
        "    pretrained=True,\n",
        "    num_classes=num_classes,\n",
        ")\n",
        "model_resnet.to(device)\n",
        "\n",
        "optimizer_resnet = AdamW(\n",
        "    model_resnet.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# Steps per epoch = batches in train loader\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_training_steps = EPOCHS * steps_per_epoch\n",
        "print(\"Steps per epoch:\", steps_per_epoch)\n",
        "print(\"Total training steps:\", total_training_steps)\n",
        "\n",
        "scheduler_resnet = create_warmup_scheduler(\n",
        "    optimizer_resnet,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        ")\n",
        "\n",
        "history_resnet = {\"epoch\": [], \"val_acc\": []}\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    global_step = train_one_epoch(\n",
        "        model_resnet,\n",
        "        train_loader,\n",
        "        optimizer_resnet,\n",
        "        scheduler_resnet,\n",
        "        epoch,\n",
        "        total_steps_done=global_step,\n",
        "        use_bf16=True,\n",
        "    )\n",
        "\n",
        "    val_acc, _ = evaluate(model_resnet, val_loader)\n",
        "    history_resnet[\"epoch\"].append(epoch)\n",
        "    history_resnet[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch} - Val Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "iB2brDGEvAU2"
      },
      "id": "iB2brDGEvAU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 - CNN"
      ],
      "metadata": {
        "id": "LrX5yJIjv8fs"
      },
      "id": "LrX5yJIjv8fs"
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_cnn = SmallCNN(num_classes=num_classes).to(device)\n",
        "\n",
        "optimizer_cnn = AdamW(\n",
        "    model_cnn.parameters(),\n",
        "    lr=LR,  # same LR as ResNet\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "scheduler_cnn = create_warmup_scheduler(\n",
        "    optimizer_cnn,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        ")\n",
        "\n",
        "history_cnn = {\"epoch\": [], \"val_acc\": []}\n",
        "\n",
        "global_step_cnn = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    global_step_cnn = train_one_epoch(\n",
        "        model_cnn,\n",
        "        train_loader,\n",
        "        optimizer_cnn,\n",
        "        scheduler_cnn,\n",
        "        epoch,\n",
        "        total_steps_done=global_step_cnn,\n",
        "        use_bf16=True,\n",
        "    )\n",
        "\n",
        "    val_acc, _ = evaluate(model_cnn, val_loader)\n",
        "    history_cnn[\"epoch\"].append(epoch)\n",
        "    history_cnn[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    print(f\"[CNN] Epoch {epoch} - Val Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "_HOW6Yof00df"
      },
      "id": "_HOW6Yof00df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Results"
      ],
      "metadata": {
        "id": "5XFLlzO01Hgf"
      },
      "id": "5XFLlzO01Hgf"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history_resnet[\"epoch\"], history_resnet[\"val_acc\"], label=\"ResNet50\")\n",
        "plt.plot(history_cnn[\"epoch\"], history_cnn[\"val_acc\"], label=\"Small CNN\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Model Comparison\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fGgznHip03rd"
      },
      "id": "fGgznHip03rd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}