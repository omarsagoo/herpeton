{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ad0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ac2630",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch HF token from .env\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n\u001b[1;32m      5\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Check if HF token is available, If it is available, HF will use it from the env automatically\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Send an error if not, however this can be ignored if using SSH access.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetch HF token from .env\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Check if HF token is available, If it is available, HF will use it from the env automatically\n",
    "# Send an error if not, however this can be ignored if using SSH access.\n",
    "if hf_token is None:\n",
    "\traise ValueError(\"HF_TOKEN not found in environment variables. Either set it in your .env file or ignore if you are using SSH access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22937cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data/biotrove_train\")\n",
    "META_DIR = DATA_ROOT / \"raw_metadata\"\n",
    "PARAQUETS_PATH = META_DIR / \"BioTrove-train\"\n",
    "OUT_DIR = DATA_ROOT / \"processed_metadata\"\n",
    "FILTERED_OUT = DATA_ROOT / \"filtered_reptilia\"\n",
    "INPUT_PARQUETS = FILTERED_OUT / \"merged_cases\"\n",
    "IMG_DIR = DATA_ROOT / \"images_reptilia\"\n",
    "SPLITS_CSV = DATA_ROOT /  \"reptilia_tar_splits.csv\"\n",
    "COUNTS = DATA_ROOT / \"processed_metadata\" / \"combined_sample_counts_per_species.csv\"\n",
    "\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_PARQUETS.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the biotrove-train metadata from huggingface\n",
    "snapshot_download(\n",
    "    repo_id=\"BGLab/BioTrove-Train\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=str(META_DIR),\n",
    "    # Only download parquet files\n",
    "    allow_patterns=[\"BioTrove-Train/*.parquet\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the parquet files into csvs\n",
    "mp = MetadataProcessor(\n",
    "    source_folder=str(PARAQUETS_PATH),\n",
    "    destination_folder=str(OUT_DIR),\n",
    "    categories=[\"Reptilia\"],\n",
    ")\n",
    "mp.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shuffled chunks with filtering\n",
    "# Overriding the class because their process method doesnt not properly filter out the data that we want it to.\n",
    "# The vast majority of this logic is copied from the original class, the only change is adding a filter to the \n",
    "# pd.read_paraquet method. Without this, it reads all data including non-reptilia data and their capped_filtered_df\n",
    "# only filters out rare cases, not rare cases and categories that are not in the species count data.\n",
    "class GenShuffledChunksReptilia(GenShuffledChunks):\n",
    "\tdef process_files(self):\n",
    "\t\t\"\"\"\n",
    "\t\tProcess files based on configuration parameters. Filters rare cases,\n",
    "\t\tcaps frequent cases, and shuffles the data into specified parts.\n",
    "\t\t\"\"\"\n",
    "\t\tstart_time = time.time()\n",
    "\t\t\n",
    "\t\tfinal_counts = pd.read_csv(self.species_count_data)\n",
    "\t\trare_case = set(final_counts[final_counts['count'] < self.rare_threshold]['species'])\n",
    "\t\tfrequent_case = set(final_counts[final_counts['count'] > self.cap_threshold]['species'])\n",
    "\n",
    "\t\tfor dir_path in [self.rare_dir, self.cap_filtered_dir_train, self.capped_dir, self.merged_dir]:\n",
    "\t\t\tif os.path.exists(dir_path):\n",
    "\t\t\t\tshutil.rmtree(dir_path)\n",
    "\t\t\tos.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\t\tfrequent_counts = {}\n",
    "\t\tcapped_cases = []\n",
    "\t\tfiles = [f for f in os.listdir(self.directory) if f.endswith(\".parquet\")]\n",
    "\n",
    "\t\tfor filename in tqdm(files, desc=\"Processing files\"):\n",
    "\t\t\tfilepath = os.path.join(self.directory, filename)\n",
    "\t\t\tdf = pd.read_parquet(filepath, filters=[(\"class\", \"==\", \"Reptilia\")]).dropna()\n",
    "\n",
    "\t\t\trare_df = df[df['species'].isin(rare_case)]\n",
    "\t\t\tcapped_filtered_df = df[~df['species'].isin(rare_case)]\n",
    "\n",
    "\t\t\trare_df.to_parquet(os.path.join(self.rare_dir, filename), index=False)\n",
    "\n",
    "\t\t\tfrequent_df = capped_filtered_df[capped_filtered_df['species'].isin(frequent_case)]\n",
    "\t\t\tfrequent_case_counts = frequent_df['species'].value_counts().to_dict()\n",
    "\n",
    "\t\t\tfor case, count in frequent_case_counts.items():\n",
    "\t\t\t\tfrequent_counts[case] = frequent_counts.get(case, 0) + count\n",
    "\t\t\t\tif frequent_counts[case] > self.cap_threshold and case not in capped_cases:\n",
    "\t\t\t\t\tcapped_cases.append(case)\n",
    "\t\t\t\t\tcap_case_df = frequent_df[frequent_df['species'] == case]\n",
    "\t\t\t\t\tcap_case_df.to_parquet(os.path.join(self.capped_dir, f'capped_{case}.parquet'), index=False)\n",
    "\n",
    "\t\t\tcapped_df = capped_filtered_df[~capped_filtered_df['species'].isin(capped_cases)]\n",
    "\t\t\tdf_shuffled = capped_df.sample(frac=1, random_state=self.random_seed).reset_index(drop=True)\n",
    "\t\t\tnum_parts = max(1, round(len(df_shuffled) / self.part_size))\n",
    "\t\t\trows_per_part = len(df_shuffled) // num_parts\n",
    "\n",
    "\t\t\tdf_parts = [df_shuffled.iloc[i * rows_per_part: (i + 1) * rows_per_part] for i in range(num_parts)]\n",
    "\n",
    "\t\t\tif len(df_shuffled) % num_parts != 0:\n",
    "\t\t\t\tdf_parts[-1] = pd.concat([df_parts[-1], df_shuffled.iloc[num_parts * rows_per_part:]], ignore_index=True)\n",
    "\n",
    "\t\t\tbase_filename, _ = os.path.splitext(filename)\n",
    "\t\t\tfor i, part in enumerate(df_parts):\n",
    "\t\t\t\tcap_filtered_filepath = os.path.join(self.cap_filtered_dir_train, f'{base_filename}_part{i+1}.parquet')\n",
    "\t\t\t\tpart.to_parquet(cap_filtered_filepath, index=False)\n",
    "\n",
    "\t\tself.merge_shuffled_files()\n",
    "\t\telapsed_time = time.time() - start_time\n",
    "\t\tprint(f\"Processing completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shuffled chunks with filtering\n",
    "# - cap species that have more than 50 samples, to only 50 samples.\n",
    "# and if a species has less than 1000 samples, it will be dropped.\n",
    "# Without the override above, the process method does not filter properly and generates the shuffled chunks\n",
    "# for all categories, resulting in over 100m rows and images to download, with it we only get 10k rows.\n",
    "# processing also goes from ~5 minutes to 15 seconds.\n",
    "gen = GenShuffledChunksReptilia(\n",
    "    species_count_data=COUNTS,\n",
    "    directory=PARAQUETS_PATH,\n",
    "    rare_threshold=1000,          # drop species with <1000 samples\n",
    "    cap_threshold=50,         # cap species above this count\n",
    "    part_size=500,              # rows per shuffled part\n",
    "    rare_dir=str(FILTERED_OUT / \"rare_cases\"),\n",
    "    cap_filtered_dir_train=str(FILTERED_OUT / \"cap_filtered_train\"),\n",
    "    capped_dir=str(FILTERED_OUT / \"capped_cases\"),\n",
    "    merged_dir=str(FILTERED_OUT / \"merged_cases\"),\n",
    "    files_per_chunk=80,\n",
    "    random_seed=521,\n",
    ")\n",
    "gen.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacec06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is just for testing and visualization to see how many rows we have in the filtered parquet files.\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq \n",
    "\n",
    "parquet_dir = INPUT_PARQUETS \n",
    "total_rows = 0\n",
    "files = sorted(Path(parquet_dir).glob(\"*.parquet\"))\n",
    "for f in files:\n",
    "    total_rows += pq.ParquetFile(f).metadata.num_rows\n",
    "\n",
    "print(f\"Parquet files: {len(files)}, total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took about 1 minute and 15 seconds to download 10k images\n",
    "gi = GetImages(\n",
    "    INPUT_PARQUETS,\n",
    "    output_folder=str(IMG_DIR),\n",
    "    concurrent_downloads=1000,\n",
    ")\n",
    "await gi.download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen = GenImgTxtPair(\n",
    "    INPUT_PARQUETS,\n",
    "    img_folder= IMG_DIR,\n",
    "    generate_tar=True,\n",
    ")\n",
    "\n",
    "textgen.create_image_text_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdabd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_images_per_class = 20\n",
    "split_ratios = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1} # 80/10/10 split\n",
    "\n",
    "train_p = split_ratios[\"train\"]\n",
    "val_p = split_ratios[\"val\"]\n",
    "\n",
    "# Check available images before splitting to ensure the data stays balanced\n",
    "available_ids = {int(p.stem) for p in IMG_DIR.rglob(\"*.jpg\")}\n",
    "print(f\"Images found: {len(available_ids)}\")\n",
    "\n",
    "dfs = []\n",
    "for p in Path(INPUT_PARQUETS).glob(\"*.parquet\"):\n",
    "    df = pd.read_parquet(p, columns=[\"photo_id\", \"scientificName\", \"photo_url\"])\n",
    "    dfs.append(df)\n",
    "    \n",
    "\n",
    "meta = pd.concat(dfs, ignore_index=True).dropna(subset=[\"photo_id\", \"scientificName\", \"photo_url\"])\n",
    "\n",
    "meta[\"photo_id\"] = meta[\"photo_id\"].astype(int)\n",
    "meta = meta[meta[\"photo_id\"].isin(available_ids)].drop_duplicates(subset=[\"photo_id\"], keep=False)\n",
    "\n",
    "print(f\"Rows with images: {len(meta[\"photo_id\"].unique())}\")\n",
    "\n",
    "class_counts = meta[\"scientificName\"].value_counts()\n",
    "keep_classes = set(class_counts[class_counts >= min_images_per_class].index)\n",
    "print(f\"Classes kept (>= {min_images_per_class} images): {len(keep_classes)}\")\n",
    "print(f\"Classes dropped: {len(class_counts) - len(keep_classes)}\")\n",
    "\n",
    "\n",
    "meta = meta[meta[\"scientificName\"].isin(keep_classes)].copy()\n",
    "print(f\"Rows after class filter: {len(meta)}\")\n",
    "\n",
    "rng = random.Random(521)\n",
    "rows = []\n",
    "for label, g in meta.groupby(\"scientificName\"):\n",
    "    ids = g[\"photo_id\"].tolist()\n",
    "    rng.shuffle(ids)\n",
    "    n = len(ids)\n",
    "    train_n = int(n * split_ratios[\"train\"])\n",
    "    val_n = int((n - train_n) // 2)\n",
    "    # val_n = int(n * split_ratios[\"val\"])\n",
    "    for pid in ids[:train_n]:\n",
    "        rows.append((pid, \"train\"))\n",
    "    for pid in ids[train_n:train_n + val_n]:\n",
    "        rows.append((pid, \"test\"))\n",
    "    for pid in ids[train_n + val_n:]:\n",
    "        rows.append((pid, \"val\"))\n",
    "        \n",
    "\n",
    "split_df = pd.DataFrame(rows, columns=[\"photo_id\", \"split\"])\n",
    "split_lookup = dict(zip(split_df.photo_id, split_df.split))\n",
    "print(split_df[\"split\"].value_counts())\n",
    "\n",
    "meta[\"split\"] = meta[\"photo_id\"].map(split_lookup)\n",
    "print(\"Rows per split:\\n\", meta[\"split\"].value_counts())\n",
    "\n",
    "per_class = meta.groupby([\"scientificName\", \"split\"]).size().unstack(fill_value=0)\n",
    "print(\"\\nPer-class counts (train/val/test) head:\\n\", per_class.head())\n",
    "\n",
    "# Simple imbalance stats\n",
    "train_counts = per_class[\"train\"]\n",
    "val_counts   = per_class[\"val\"]\n",
    "test_counts  = per_class[\"test\"]\n",
    "print(f\"\\nTrain min/median/max: {train_counts.min()} / {train_counts.median()} / {train_counts.max()}\")\n",
    "print(f\"Val   min/median/max: {val_counts.min()} / {val_counts.median()} / {val_counts.max()}\")\n",
    "print(f\"Test  min/median/max: {test_counts.min()} / {test_counts.median()} / {test_counts.max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
