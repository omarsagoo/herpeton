{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4697ce0",
   "metadata": {},
   "source": [
    "# Herpeton Project\n",
    "\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: [BGLab/BioTrove-Train on Hugging Face](https://huggingface.co/datasets/BGLab/BioTrove-Train)\n",
    "- **Focus**: Reptilia taxonomic class (snakes, lizards, turtles, etc.)\n",
    "- **Total Dataset Size**: ~135M samples across 7 taxonomic groups\n",
    "- **Estimated Reptilia Subset**: ~1.3M labeled reptile images across 189+ species\n",
    "- **Official Processing**: Uses `arbor_process` library for metadata preprocessing\n",
    "\n",
    "## Setup Requirements\n",
    "1. Install required packages\n",
    "2. Download metadata files from HuggingFace\n",
    "3. Process using official BioTrove tools\n",
    "4. Download images and create ML-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e2cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up BioTrove Reptilia processing environment...\n",
      "============================================================\n",
      "Checking PyArrow compatibility...\n",
      "PyArrow version: 14.0.2\n",
      "PyExtensionType is available\n",
      "\n",
      "============================================================\n",
      "arbor-process is already installed\n",
      "nest_asyncio is already installed\n",
      "datasets>=2.14.0 is already installed\n",
      "pandas>=1.5.0 is already installed\n",
      "numpy is already installed\n",
      "matplotlib is already installed\n",
      "seaborn is already installed\n",
      "pillow is already installed\n",
      "requests is already installed\n",
      "tqdm is already installed\n",
      "\n",
      "All packages installed successfully\n",
      "Ready to process BioTrove Reptilia dataset\n",
      "\n",
      "NOTE: If PyArrow errors persist, try:\n",
      "   pip install --force-reinstall pyarrow==14.0.2\n",
      "   pip install --force-reinstall pandas==2.0.3\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install and address any PyArrow compatibility issues\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def check_and_fix_pyarrow():\n",
    "    \"\"\"Check PyArrow version and fix compatibility issues\"\"\"\n",
    "    print(\"Checking PyArrow compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        print(f\"PyArrow version: {pa.__version__}\")\n",
    "        \n",
    "        # Test for the extension type attribute\n",
    "        if hasattr(pa.lib, 'PyExtensionType'):\n",
    "            print(\"PyExtensionType is available\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"PyExtensionType not found - version issue detected\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"PyArrow not installed\")\n",
    "        return False\n",
    "\n",
    "def fix_pyarrow_compatibility():\n",
    "    \"\"\"Fix PyArrow version compatibility\"\"\"\n",
    "    print(\"\\nFixing PyArrow compatibility...\")\n",
    "    \n",
    "    try:\n",
    "        # Uninstall and reinstall with specific compatible versions if needed\n",
    "        print(\"Uninstalling existing PyArrow...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"])\n",
    "        \n",
    "        print(\"Installing compatible PyArrow version...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow>=12.0.0,<15.0.0\"])\n",
    "        \n",
    "        # Ensure pandas compatibility\n",
    "        print(\"Updating pandas for compatibility...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pandas>=1.5.0\"])\n",
    "        \n",
    "        print(\"PyArrow compatibility fix complete!\")\n",
    "        return True\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to fix PyArrow: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_package(package_name, import_name=None):\n",
    "    \"\"\"Install a package and verify it can be imported\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        # Try to import the package\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"{package_name} is already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"{package_name} installed successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"Setting up BioTrove Reptilia processing environment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check and fix PyArrow if needed\n",
    "if not check_and_fix_pyarrow():\n",
    "    print(\"\\nAttempting to fix PyArrow compatibility...\")\n",
    "    if fix_pyarrow_compatibility():\n",
    "        # Re-check after fix\n",
    "        if check_and_fix_pyarrow():\n",
    "            print(\"PyArrow issue resolved!\")\n",
    "        else:\n",
    "            print(\"WARNING: PyArrow issue persists\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not automatically fix PyArrow\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# List of required packages (with specific versions to avoid conflicts)\n",
    "required_packages = [\n",
    "    (\"arbor-process\", \"arbor_process\"),\n",
    "    (\"nest_asyncio\", \"nest_asyncio\"),\n",
    "    (\"datasets>=2.14.0\", \"datasets\"),\n",
    "    (\"pandas>=1.5.0\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"pillow\", \"PIL\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "all_installed = True\n",
    "for package_name, import_name in required_packages:\n",
    "    if not install_package(package_name, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nAll packages installed successfully\")\n",
    "    print(\"Ready to process BioTrove Reptilia dataset\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Some packages failed to install\")\n",
    "    print(\"You may need to install them manually\")\n",
    "\n",
    "print(\"\\nNOTE: If PyArrow errors persist, try:\")\n",
    "print(\"   pip install --force-reinstall pyarrow==14.0.2\")\n",
    "print(\"   pip install --force-reinstall pandas==2.0.3\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639411f7",
   "metadata": {},
   "source": [
    "## Step 1: Extract Reptilia Metadata from BioTrove\n",
    "\n",
    "Using HuggingFace's efficient filter() method to extract only Reptilia samples from the 135M sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFICIENT REPTILIA EXTRACTION\n",
      "============================================================\n",
      "Using filter() to extract only Reptilia class\n",
      "This loads and filters data in chunks efficiently\n",
      "\n",
      "Loading BioTrove-Train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c8a794a8243cda80062d0809d9473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Reptilia filter...\n",
      "Extracting Reptilia samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88e86fb1fbb4a3faa085a2d605759f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Reptilia: 0 samples [00:00, ? samples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Sample 1:\n",
      "      scientificName: Caiman yacare\n",
      "      family: Alligatoridae\n",
      "      order: Crocodylia\n",
      "\n",
      "   Sample 2:\n",
      "      scientificName: Eretmochelys imbricata\n",
      "      family: Cheloniidae\n",
      "      order: Testudines\n",
      "\n",
      "   Sample 3:\n",
      "      scientificName: Eretmochelys imbricata\n",
      "      family: Cheloniidae\n",
      "      order: Testudines\n",
      "\n",
      "   Sample 4:\n",
      "      scientificName: Chelonia mydas\n",
      "      family: Cheloniidae\n",
      "      order: Testudines\n",
      "\n",
      "   Sample 5:\n",
      "      scientificName: Chelonia mydas\n",
      "      family: Cheloniidae\n",
      "      order: Testudines\n",
      "\n",
      "   Sample 6:\n",
      "      scientificName: Chelonia mydas\n",
      "      family: Cheloniidae\n",
      "      order: Testudines\n",
      "\n",
      "   Sample 7:\n",
      "      scientificName: Bothrops atrox\n",
      "      family: Viperidae\n",
      "      order: Squamata\n",
      "\n",
      "   Sample 8:\n",
      "      scientificName: Bothrops atrox\n",
      "      family: Viperidae\n",
      "      order: Squamata\n",
      "\n",
      "   Sample 9:\n",
      "      scientificName: Erythrolamprus aesculapii\n",
      "      family: Colubridae\n",
      "      order: Squamata\n",
      "\n",
      "   Sample 10:\n",
      "      scientificName: Erythrolamprus aesculapii\n",
      "      family: Colubridae\n",
      "      order: Squamata\n",
      "\n",
      "   Reached target of 2000 samples\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE:\n",
      "   Total Reptilia samples collected: 2000\n",
      "\n",
      "   Saving 2000 samples in chunks of 250...\n",
      "      Chunk 0: 250 samples -> biotrove_metadata/reptilia_chunk_0.parquet\n",
      "      Chunk 1: 250 samples -> biotrove_metadata/reptilia_chunk_1.parquet\n",
      "      Chunk 2: 250 samples -> biotrove_metadata/reptilia_chunk_2.parquet\n",
      "      Chunk 3: 250 samples -> biotrove_metadata/reptilia_chunk_3.parquet\n",
      "      Chunk 4: 250 samples -> biotrove_metadata/reptilia_chunk_4.parquet\n",
      "      Chunk 5: 250 samples -> biotrove_metadata/reptilia_chunk_5.parquet\n",
      "      Chunk 6: 250 samples -> biotrove_metadata/reptilia_chunk_6.parquet\n",
      "      Chunk 7: 250 samples -> biotrove_metadata/reptilia_chunk_7.parquet\n",
      "\n",
      "   ✓ Saved 8 parquet files\n",
      "   ✓ 463 unique species\n",
      "\n",
      "   Top 10 families:\n",
      "      Colubridae: 400 samples\n",
      "      Phrynosomatidae: 189 samples\n",
      "      Scincidae: 176 samples\n",
      "      Gekkonidae: 121 samples\n",
      "      Viperidae: 98 samples\n",
      "      Agamidae: 89 samples\n",
      "      Cheloniidae: 67 samples\n",
      "      Teiidae: 63 samples\n",
      "      Iguanidae: 62 samples\n",
      "      Anolidae: 61 samples\n",
      "\n",
      "   Reached target of 2000 samples\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE:\n",
      "   Total Reptilia samples collected: 2000\n",
      "\n",
      "   Saving 2000 samples in chunks of 250...\n",
      "      Chunk 0: 250 samples -> biotrove_metadata/reptilia_chunk_0.parquet\n",
      "      Chunk 1: 250 samples -> biotrove_metadata/reptilia_chunk_1.parquet\n",
      "      Chunk 2: 250 samples -> biotrove_metadata/reptilia_chunk_2.parquet\n",
      "      Chunk 3: 250 samples -> biotrove_metadata/reptilia_chunk_3.parquet\n",
      "      Chunk 4: 250 samples -> biotrove_metadata/reptilia_chunk_4.parquet\n",
      "      Chunk 5: 250 samples -> biotrove_metadata/reptilia_chunk_5.parquet\n",
      "      Chunk 6: 250 samples -> biotrove_metadata/reptilia_chunk_6.parquet\n",
      "      Chunk 7: 250 samples -> biotrove_metadata/reptilia_chunk_7.parquet\n",
      "\n",
      "   ✓ Saved 8 parquet files\n",
      "   ✓ 463 unique species\n",
      "\n",
      "   Top 10 families:\n",
      "      Colubridae: 400 samples\n",
      "      Phrynosomatidae: 189 samples\n",
      "      Scincidae: 176 samples\n",
      "      Gekkonidae: 121 samples\n",
      "      Viperidae: 98 samples\n",
      "      Agamidae: 89 samples\n",
      "      Cheloniidae: 67 samples\n",
      "      Teiidae: 63 samples\n",
      "      Iguanidae: 62 samples\n",
      "      Anolidae: 61 samples\n"
     ]
    }
   ],
   "source": [
    "def extract_reptilia_efficient():\n",
    "    \"\"\"\n",
    "    Efficiently extract Reptilia samples using HuggingFace datasets with filtering.\n",
    "    Since Reptilia exists in the dataset but is distributed throughout 135M samples,\n",
    "    the dataset's built-in filtering capabilities are used.\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(\"EFFICIENT REPTILIA EXTRACTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Using filter() to extract only Reptilia class\")\n",
    "    print(\"This loads and filters data in chunks efficiently\")\n",
    "    print(\"Loads and filters data in chunks efficiently\")\n",
    "    \n",
    "    # Load dataset in streaming mode\n",
    "    print(\"Loading BioTrove-Train dataset...\")\n",
    "    dataset = load_dataset('BGLab/BioTrove-Train', streaming=True, split='train')\n",
    "    \n",
    "    # Filter for Reptilia class\n",
    "    print(\"Applying Reptilia filter...\")\n",
    "    reptilia_dataset = dataset.filter(lambda example: example['class'] == 'Reptilia')\n",
    "    \n",
    "    print(\"Extracting Reptilia samples...\")\n",
    "    reptilia_samples = []\n",
    "    \n",
    "    pbar = tqdm(desc=\"Collecting Reptilia\", unit=\" samples\")\n",
    "    \n",
    "    # Collect samples\n",
    "    target_samples = 2000  # Collect 2000 Reptilia samples\n",
    "    \n",
    "    for idx, item in enumerate(reptilia_dataset):\n",
    "        reptilia_samples.append(item)\n",
    "        \n",
    "        if idx < 10:  # Show first 10\n",
    "            print(f\"\\n   Sample {idx+1}:\")\n",
    "            print(f\"      scientificName: {item.get('scientificName', 'Unknown')}\")\n",
    "            print(f\"      family: {item.get('family', 'Unknown')}\")\n",
    "            print(f\"      order: {item.get('order', 'Unknown')}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Collected {len(reptilia_samples)} Reptilia\")\n",
    "        \n",
    "        if len(reptilia_samples) >= target_samples:\n",
    "            print(f\"\\n   Reached target of {target_samples} samples\")\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTION COMPLETE:\")\n",
    "    print(f\"   Total Reptilia samples collected: {len(reptilia_samples)}\")\n",
    "    \n",
    "    if reptilia_samples:\n",
    "        # Save to parquet chunks\n",
    "        os.makedirs(\"biotrove_metadata\", exist_ok=True)\n",
    "        chunk_size = 250\n",
    "        saved_files = []\n",
    "        \n",
    "        print(f\"\\n   Saving {len(reptilia_samples)} samples in chunks of {chunk_size}...\")\n",
    "        \n",
    "        for i in range(0, len(reptilia_samples), chunk_size):\n",
    "            chunk_data = reptilia_samples[i:i+chunk_size]\n",
    "            chunk_df = pd.DataFrame(chunk_data)\n",
    "            \n",
    "            chunk_filename = f\"biotrove_metadata/reptilia_chunk_{i//chunk_size}.parquet\"\n",
    "            chunk_df.to_parquet(chunk_filename, index=False)\n",
    "            saved_files.append(chunk_filename)\n",
    "            \n",
    "            print(f\"      Chunk {i//chunk_size}: {len(chunk_data)} samples -> {chunk_filename}\")\n",
    "        \n",
    "        print(f\"\\n   ✓ Saved {len(saved_files)} parquet files\")\n",
    "        \n",
    "        # Show species diversity\n",
    "        species_list = [s.get('scientificName', 'Unknown') for s in reptilia_samples]\n",
    "        unique_species = set(species_list)\n",
    "        print(f\"   ✓ {len(unique_species)} unique species\")\n",
    "        \n",
    "        # Show top families\n",
    "        families = [s.get('family', 'Unknown') for s in reptilia_samples]\n",
    "        from collections import Counter\n",
    "        top_families = Counter(families).most_common(10)\n",
    "        print(f\"\\n   Top 10 families:\")\n",
    "        for family, count in top_families:\n",
    "            print(f\"      {family}: {count} samples\")\n",
    "        \n",
    "        return {\n",
    "            'samples': reptilia_samples,\n",
    "            'saved_files': saved_files,\n",
    "            'unique_species': len(unique_species)\n",
    "        }\n",
    "    else:\n",
    "        print(\"   ✗ No Reptilia samples found\")\n",
    "        return None\n",
    "\n",
    "# Run the efficient extraction\n",
    "reptilia_data = extract_reptilia_efficient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522339e",
   "metadata": {},
   "source": [
    "## Step 2: Verify Extracted Data\n",
    "\n",
    "Check the extracted Reptilia samples before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: Extracted Reptilia Data\n",
      "============================================================\n",
      "Found 8 parquet chunk files:\n",
      "   biotrove_metadata/reptilia_chunk_0.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_1.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_2.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_3.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_4.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_5.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_6.parquet: 250 samples\n",
      "   biotrove_metadata/reptilia_chunk_7.parquet: 250 samples\n",
      "\n",
      "Loading all Reptilia data...\n",
      "\n",
      "TOTAL REPTILIA DATASET:\n",
      "   Total samples: 2,000\n",
      "   Unique species: 463\n",
      "   Unique families: 51\n",
      "   Unique orders: 4\n",
      "\n",
      "COLUMNS:\n",
      "   ['photo_id', 'scientificName', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'common_name', 'taxonRank', 'photo_url']\n",
      "\n",
      "ORDER DISTRIBUTION:\n",
      "   Squamata: 1678 samples (83.9%)\n",
      "   Testudines: 235 samples (11.8%)\n",
      "   Crocodylia: 84 samples (4.2%)\n",
      "   Rhynchocephalia: 3 samples (0.1%)\n",
      "\n",
      "TOP 15 SPECIES:\n",
      "   Sceloporus occidentalis                  (Phrynosomatidae     ): 89 samples\n",
      "   Chelonia mydas                           (Cheloniidae         ): 45 samples\n",
      "   Iguana iguana                            (Iguanidae           ): 44 samples\n",
      "   Alligator mississippiensis               (Alligatoridae       ): 43 samples\n",
      "   Lampropeltis californiae                 (Colubridae          ): 39 samples\n",
      "   Phrynosoma blainvillii                   (Phrynosomatidae     ): 30 samples\n",
      "   Chelydra serpentina                      (Chelydridae         ): 29 samples\n",
      "   Anolis carolinensis                      (Anolidae            ): 27 samples\n",
      "   Elgaria multicarinata                    (Anguidae            ): 24 samples\n",
      "   Actinemys marmorata                      (Emydidae            ): 22 samples\n",
      "   Pogona barbata                           (Agamidae            ): 22 samples\n",
      "   Plestiodon skiltonianus                  (Scincidae           ): 22 samples\n",
      "   Kentropyx calcarata                      (Teiidae             ): 20 samples\n",
      "   Aspidoscelis tigris                      (Teiidae             ): 20 samples\n",
      "   Thamnophis elegans                       (Colubridae          ): 19 samples\n",
      "\n",
      "FIRST 3 SAMPLE ROWS:\n",
      " photo_id         scientificName  kingdom   phylum    class      order        family        genus   species   common_name taxonRank                                                             photo_url\n",
      "    21332          Caiman yacare Animalia Chordata Reptilia Crocodylia Alligatoridae       Caiman    yacare Yacare Caiman   species http://inaturalist-open-data.s3.amazonaws.com/photos/21332/medium.jpg\n",
      "    22094 Eretmochelys imbricata Animalia Chordata Reptilia Testudines   Cheloniidae Eretmochelys imbricata     Hawksbill   species http://inaturalist-open-data.s3.amazonaws.com/photos/22094/medium.JPG\n",
      "    22585 Eretmochelys imbricata Animalia Chordata Reptilia Testudines   Cheloniidae Eretmochelys imbricata     Hawksbill   species http://inaturalist-open-data.s3.amazonaws.com/photos/22585/medium.JPG\n",
      "\n",
      "============================================================\n",
      "✓ Reptilia dataset successfully extracted and verified!\n",
      "✓ Ready for BioTrove processing pipeline\n"
     ]
    }
   ],
   "source": [
    "# Verify the extracted Reptilia data\n",
    "import glob\n",
    "\n",
    "print(\"VERIFICATION: Extracted Reptilia Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all reptilia chunk files\n",
    "chunk_files = sorted(glob.glob(\"biotrove_metadata/reptilia_chunk_*.parquet\"))\n",
    "\n",
    "print(f\"Found {len(chunk_files)} parquet chunk files:\")\n",
    "for file in chunk_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    print(f\"   {file}: {len(df)} samples\")\n",
    "\n",
    "# Load all chunks\n",
    "print(\"\\nLoading all Reptilia data...\")\n",
    "all_dfs = [pd.read_parquet(f) for f in chunk_files]\n",
    "reptilia_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTOTAL REPTILIA DATASET:\")\n",
    "print(f\"   Total samples: {len(reptilia_df):,}\")\n",
    "print(f\"   Unique species: {reptilia_df['scientificName'].nunique()}\")\n",
    "print(f\"   Unique families: {reptilia_df['family'].nunique()}\")\n",
    "print(f\"   Unique orders: {reptilia_df['order'].nunique()}\")\n",
    "\n",
    "print(f\"\\nCOLUMNS:\")\n",
    "print(f\"   {list(reptilia_df.columns)}\")\n",
    "\n",
    "print(f\"\\nORDER DISTRIBUTION:\")\n",
    "order_counts = reptilia_df['order'].value_counts()\n",
    "for order, count in order_counts.items():\n",
    "    print(f\"   {order}: {count} samples ({count/len(reptilia_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTOP 15 SPECIES:\")\n",
    "species_counts = reptilia_df['scientificName'].value_counts().head(15)\n",
    "for species, count in species_counts.items():\n",
    "    family = reptilia_df[reptilia_df['scientificName'] == species]['family'].iloc[0]\n",
    "    print(f\"   {species:<40} ({family:<20}): {count} samples\")\n",
    "\n",
    "print(f\"\\nFIRST 3 SAMPLE ROWS:\")\n",
    "print(reptilia_df.head(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ Reptilia dataset successfully extracted and verified!\")\n",
    "print(\"✓ Ready for BioTrove processing pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89dac30",
   "metadata": {},
   "source": [
    "## Step 3: Download Images and Create Dataset\n",
    "\n",
    "Download images from URLs and create ML-ready dataset with image-text pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c4cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arbor_process is already installed\n"
     ]
    }
   ],
   "source": [
    "# Install arbor_process library for BioTrove processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import arbor_process\n",
    "    print(\"arbor_process is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing arbor_process library...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"arbor-process\"])\n",
    "    import arbor_process\n",
    "    print(\"Successfully installed arbor_process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aea5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPTILIA IMAGE DOWNLOAD\n",
      "============================================================\n",
      "Loading 8 Reptilia parquet chunks...\n",
      "Total Reptilia samples: 2,000\n",
      "Unique species: 463\n",
      "\n",
      "Saved combined metadata to: biotrove_processed/reptilia_metadata.csv\n",
      "\n",
      "============================================================\n",
      "DOWNLOADING IMAGES\n",
      "============================================================\n",
      "Downloading 2000 images with 4 parallel workers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979caa61229d4f0d8bf374c53adacec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DOWNLOAD COMPLETE!\n",
      "  Successfully downloaded: 1985 images\n",
      "  Failed downloads: 15 images\n",
      "  Success rate: 99.2%\n",
      "\n",
      "============================================================\n",
      "CREATING IMAGE-TEXT PAIRS\n",
      "============================================================\n",
      "Created 1985 image-text pairs\n",
      "Saved to: biotrove_processed/reptilia_dataset_final.csv\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE!\n",
      "  Metadata: biotrove_processed/reptilia_metadata.csv\n",
      "  Images: biotrove_processed/images/ (1985 files)\n",
      "  Final dataset: biotrove_processed/reptilia_dataset_final.csv\n",
      "  Dataset ready for computer vision training!\n"
     ]
    }
   ],
   "source": [
    "# Download Reptilia images from metadata URLs\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"REPTILIA IMAGE DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"biotrove_processed\", exist_ok=True)\n",
    "os.makedirs(\"biotrove_processed/images\", exist_ok=True)\n",
    "\n",
    "# Load all Reptilia chunks\n",
    "chunk_files = sorted(glob.glob(\"biotrove_metadata/reptilia_chunk_*.parquet\"))\n",
    "print(f\"Loading {len(chunk_files)} Reptilia parquet chunks...\")\n",
    "\n",
    "all_chunks = [pd.read_parquet(f) for f in chunk_files]\n",
    "reptilia_metadata = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Total Reptilia samples: {len(reptilia_metadata):,}\")\n",
    "print(f\"Unique species: {reptilia_metadata['scientificName'].nunique()}\")\n",
    "\n",
    "# Save combined metadata\n",
    "metadata_path = \"biotrove_processed/reptilia_metadata.csv\"\n",
    "reptilia_metadata.to_csv(metadata_path, index=False)\n",
    "print(f\"\\nSaved combined metadata to: {metadata_path}\")\n",
    "\n",
    "def download_image(row):\n",
    "    \"\"\"Download a single image from URL\"\"\"\n",
    "    photo_id = row['photo_id']\n",
    "    \"\"\"Download single image from URL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(photo_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Open and verify image\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        \n",
    "        # Save image\n",
    "        img_filename = f\"biotrove_processed/images/{photo_id}.jpg\"\n",
    "        img.convert('RGB').save(img_filename, 'JPEG', quality=95)\n",
    "        \n",
    "        return {'photo_id': photo_id, 'status': 'success', 'path': img_filename}\n",
    "    except Exception as e:\n",
    "        return {'photo_id': photo_id, 'status': 'failed', 'error': str(e)}\n",
    "\n",
    "# Download images with parallel processing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNLOADING IMAGES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Downloading {len(reptilia_metadata)} images with 4 parallel workers...\")\n",
    "\n",
    "successful_downloads = []\n",
    "failed_downloads = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit all download tasks\n",
    "    futures = {executor.submit(download_image, row): idx \n",
    "               for idx, row in reptilia_metadata.iterrows()}\n",
    "    \n",
    "    # Process completed downloads with progress bar\n",
    "    with tqdm(total=len(futures), desc=\"Downloading\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                successful_downloads.append(result)\n",
    "            else:\n",
    "                failed_downloads.append(result)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Downloaded: {len(successful_downloads)}, Failed: {len(failed_downloads)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DOWNLOAD COMPLETE!\")\n",
    "print(f\"  Successfully downloaded: {len(successful_downloads)} images\")\n",
    "print(f\"  Failed downloads: {len(failed_downloads)} images\")\n",
    "print(f\"  Success rate: {len(successful_downloads)/len(reptilia_metadata)*100:.1f}%\")\n",
    "\n",
    "# Create image-text pairs dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING IMAGE-TEXT PAIRS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful_photo_ids = [d['photo_id'] for d in successful_downloads]\n",
    "successful_metadata = reptilia_metadata[reptilia_metadata['photo_id'].isin(successful_photo_ids)].copy()\n",
    "\n",
    "# Add image paths\n",
    "successful_metadata['image_path'] = successful_metadata['photo_id'].apply(\n",
    "    lambda x: f\"biotrove_processed/images/{x}.jpg\"\n",
    ")\n",
    "\n",
    "# Create text descriptions\n",
    "successful_metadata['text_description'] = successful_metadata.apply(\n",
    "    lambda row: f\"{row['scientificName']} ({row['common_name']}) - {row['family']}, Order {row['order']}, Class Reptilia\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save final dataset\n",
    "final_dataset_path = \"biotrove_processed/reptilia_dataset_final.csv\"\n",
    "successful_metadata.to_csv(final_dataset_path, index=False)\n",
    "\n",
    "print(f\"Created {len(successful_metadata)} image-text pairs\")\n",
    "print(f\"Saved to: {final_dataset_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE!\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"  Images: biotrove_processed/images/ ({len(successful_downloads)} files)\")\n",
    "print(f\"  Final dataset: {final_dataset_path}\")\n",
    "print(f\"  Dataset ready for computer vision training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED DATASET EXPLORATION\n",
      "============================================================\n",
      "\n",
      "Directory structure:\n",
      "biotrove_processed/\n",
      "  reptilia_metadata.csv\n",
      "  sample_structure.parquet_sample_counts_per_species.csv\n",
      "  reptilia_dataset_final.csv\n",
      "  combined_sample_counts_per_species.csv\n",
      "  reptilia_metadata.parquet\n",
      "  image_text_pairs/\n",
      "  images/\n",
      "    58365.jpg\n",
      "    53230.jpg\n",
      "    58371.jpg\n",
      "    28696.jpg\n",
      "    342769170.jpg\n",
      "    ... and 1973 more files\n",
      "\n",
      "============================================================\n",
      "Dataset ready for computer vision training!\n"
     ]
    }
   ],
   "source": [
    "# Explore the processed dataset\n",
    "import random\n",
    "\n",
    "print(\"PROCESSED DATASET EXPLORATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check directory structure\n",
    "print(\"\\nDirectory structure:\")\n",
    "for root, dirs, files in os.walk(\"biotrove_processed\"):\n",
    "    level = root.replace(\"biotrove_processed\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files)-5} more files\")\n",
    "\n",
    "# Load processed metadata\n",
    "if os.path.exists(\"biotrove_processed/metadata.csv\"):\n",
    "    processed_df = pd.read_csv(\"biotrove_processed/metadata.csv\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSED DATASET STATISTICS:\")\n",
    "    print(f\"  Total samples: {len(processed_df):,}\")\n",
    "    print(f\"  Unique species: {processed_df['scientificName'].nunique()}\")\n",
    "    print(f\"  Unique families: {processed_df['family'].nunique()}\")\n",
    "    print(f\"  Unique orders: {processed_df['order'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\nOrder distribution:\")\n",
    "    for order, count in processed_df['order'].value_counts().items():\n",
    "        print(f\"  {order}: {count} samples ({count/len(processed_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 10 species:\")\n",
    "    for species, count in processed_df['scientificName'].value_counts().head(10).items():\n",
    "        print(f\"  {species}: {count} samples\")\n",
    "    \n",
    "    # Show sample images if available\n",
    "    image_dir = \"biotrove_processed/images\"\n",
    "    if os.path.exists(image_dir):\n",
    "        image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if image_files:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Sample images available: {len(image_files)}\")\n",
    "            \n",
    "            # Display a few random samples\n",
    "            sample_images = random.sample(image_files, min(3, len(image_files)))\n",
    "            print(f\"\\nDisplaying {len(sample_images)} random samples...\")\n",
    "            \n",
    "            for img_file in sample_images:\n",
    "                img_path = os.path.join(image_dir, img_file)\n",
    "                # Get metadata for this image\n",
    "                img_id = os.path.splitext(img_file)[0]\n",
    "                if img_id.isdigit():\n",
    "                    sample_info = processed_df[processed_df['photo_id'] == int(img_id)]\n",
    "                    if not sample_info.empty:\n",
    "                        print(f\"\\n  Image: {img_file}\")\n",
    "                        print(f\"    Species: {sample_info.iloc[0]['scientificName']}\")\n",
    "                        print(f\"    Family: {sample_info.iloc[0]['family']}\")\n",
    "                        print(f\"    Common name: {sample_info.iloc[0].get('common_name', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Dataset ready for computer vision training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai521-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
